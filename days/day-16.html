<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Day 16 of 100 Days of CUDA - Day 16 Content">
    <meta name="theme-color" content="#4361ee">
    <title>Day 16: Day 16 Content - 100 Days of CUDA</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../css/styles.css">
    <link rel="shortcut icon" href="../assets/favicon.svg" type="image/svg+xml">
    <style>
        /* Add specific styles for code display */
        pre {
            background-color: var(--code-bg);
            padding: 1rem;
            border-radius: 4px;
            overflow-x: auto;
        }
        
        code {
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            font-size: 0.9rem;
            color: var(--text-color);
        }
        
        h2, h3, h4 {
            color: var(--primary-color);
            margin-top: 1.5rem;
        }
        
        ul, ol {
            padding-left: 2rem;
            margin-bottom: 1rem;
        }
        
        li {
            margin-bottom: 0.5rem;
        }
        
        a {
            color: var(--primary-color);
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        @media (max-width: 768px) {
            pre, code {
                font-size: 0.85rem;
            }
            
            h2 {
                font-size: 1.5rem;
            }
            
            h3 {
                font-size: 1.3rem;
            }
            
            h4 {
                font-size: 1.1rem;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <div class="header-content">
                <h1><a href="../index.html" style="text-decoration: none; color: inherit;">100 Days of CUDA</a></h1>
                <button id="theme-toggle" aria-label="Toggle dark/light mode">
                    <span class="theme-icon"></span>
                </button>
            </div>
        </header>
        
        <div class="content-wrapper">
            <aside class="sidebar">
                <div class="sidebar-header">
                    <h2>Days</h2>
                    <button id="mobile-menu-toggle" aria-label="Toggle menu">
                        <span class="menu-icon"></span>
                    </button>
                </div>
                <nav id="days-navigation">
                    <!-- Day links will be generated here -->
                    <div class="days-list-placeholder">Loading...</div>
                </nav>
            </aside>
            
            <main>
                <!-- Day content -->
                <div class="day-content">
                    <h2>Day 16</h2><br><p>#### Code: `NaiveBayes.cu`, `NaiveBayesKernel.cuh`, `NaiveBayesTrain.cuh`, `NaiveBayesTrain.cpp`, and `main.cpp`</p><h4>Summary:</h4><p>Implemented a CUDA-accelerated Naive Bayes classifier, focusing on the training and inference stages. Leveraging shared memory to maximize computational efficiency, the implementation is structured to divide work among threads for parallelized data processing of feature probabilities.</p><br><h4>Components Developed:</h4><ul><li>**`NaiveBayes.cu`**:  </li><p>   - This file contains the CUDA kernel responsible for calculating feature likelihoods and class probabilities in parallel. Shared memory was used where possible to minimize global memory access penalties.</p><p>   - Optimized kernel launches to balance between grid and block dimensions for datasets with high dimensionality. </p><br><li>**`NaiveBayesKernel.cuh`**:  </li><p>   - Header file declaring the kernel functions, ensuring modularity and separation of concerns in code structure.    </p><br><li>**`NaiveBayesTrain.cuh`**:  </li><p>   - Declared the host-side training function, encapsulating the logic to copy data to the GPU, launch CUDA kernels, and retrieve results.  </p><br><li>**`NaiveBayesTrain.cpp`**:  </li><p>   - Implemented the host-side training process, providing pre-processing for input data and managing memory transfers between CPU and GPU.</p><br><li>**`main.cpp`**:  </li><p>   - Entry point of the program, performing tasks like loading data, splitting datasets for training and testing, and evaluating model performance after training.  </p><br><hr><br><p>#### Blog Update  </p><li>Updated My blog with an important information about using NVCC in Colab. </li><li style="margin-left: 20px;">Link: <a href="https://hamdi.bearblog.dev/learning-cuda-with-a-weak-gpu-or-no-gpu-at-all-yes-you-can/" target="_blank">Learning CUDA with a Weak GPU (or No GPU at All)</a></li></ul><br><hr>
                </div>
            </main>
        </div>
        
        <footer>
            <p>&copy; <span id="current-year"></span> 100 Days of CUDA</p>
        </footer>
    </div>
    
    <script src="../js/app.js"></script>
</body>
</html>